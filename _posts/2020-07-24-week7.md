---
layout: post
title: Week 7
---

My focus this week was on creating and editing more templates. Although that was a lot of work, and very important work for my project, there is not much to write about for a blog post. Therefore, this week I want to focus on some of the interesting talks I’ve been attending.

Throughout the summer, I’ve attended guest speakers in the HCI department, the SVL department, and through Micheal Bernstein’s lab. One of the benefits of working virtually is that we have been able to have a much wider variety of people come speak, since they do not have to travel. I’ve loved some of the talks, and the ones this week I found especially interesting. The first talk [1] was about how visual analogies affect a user’s perception of a machine learning system. They were curious why different AI systems (to use the term loosely) were received differently by their audience, and they expected part of it was how they were presented. They experimented with presenting a system using analogies that projected different levels of “warmth” and “competence”. For example, a toddler is medium-high in warmth and low in competence. They then judged people’s receptivity to their experience, and their perceived likelihood to use the system again. What they found is that people are more likely to initially try out a system if it is presented as high warmth and high competence, but after testing it they are more likely to want to continue using it if it is presented as high warmth and low competence. It was interesting seeing how much a simple change in word affected how people interacted with and experienced an AI system. The second talk was by Jason Hong [2] of CMU about a variety of his work. He focuses on how to encourage end users to implement cybersecurity measures. One of the main topics he discussed was observability. One of the psychological factors that increases one’s likelihood to adopt a system is if it is observable that your social circle is also adopting it or has suffered from not adopting it. However, cyber security measures are rarely observable or a part of people’s conversations, so his work centers are creating more observable security measures. I liked both these talks especially because they involved the interaction between these tools and the people using them. It’s been nice getting to see both the Computer Vision and HCI talks. 

I am currently most interested in an interdisciplinary field between Computer Vision and HCI. However, I had noticed in meetings with Ranjay and his advisors that when developing his job talk everything had to be presented in an “HCI” way or a “Computer Vision” way. Although this is essentially an extension of how I have adjusted my resume to highlight design vs computer science skills, it was strange to me that the lanes were so defined even for research, which is supposed to be exploring new areas. I brought it up with Ranjay, and he explained how you’re positioning yourself to be hired within a certain department, so you have to connect to the faculty and contribute to that area. However, that also made me think about the presentation I saw on Critical Race Theory in HCI, and how the presenters said their ideas were often dismissed because they weren’t an “HCI” problem, mostly because no one had worked on it before. They also discussed how even if there are now more diverse researchers in the field, if research is only accepted within the confines of what was historically accepted, the shape of that conversation was shaped by the largest demographic in that field, namely the wealthy white men. Restricting what counts as valid research within a field as what previous eras researchers found important perpetuates that imbalance. However, if there aren’t the structures to support that work, it will be increasingly difficult to progress in that area. Since I’m also interested in the intersection between two fields, I wonder what can be done to support work that doesn’t fit narrowly within one department but still makes important contributions. 



[1] Conceptual Metaphors Impact Perceptions of Human-AI Collaboration (https://arxiv.org/pdf/2008.02311.pdf)

[2] Jason Hong (http://www.cs.cmu.edu/~jasonh/)
