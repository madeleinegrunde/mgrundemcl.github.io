---
layout: post
title: Week 3
---

My main goal this week was to finally pick a project for the summer. We decided to move past just reading papers and listening to presentations, and to begin playing around with the dataset and seeing if our predictions are correct. Specifically, we wanted to know if a commonsense understanding of physical attributes could improve performance on question answering tasks.

I sorted through the VQA2.0 dataset and found the relative performance of different kinds of questions that referred to different physical attributes ([Goyal et. al, 2017](https://arxiv.org/pdf/1612.00837.pdf)). I first sorted through the questions by words pertaining to different attributes like size, weight, and speed. I then looked for questions about affordances, like those that included the word “could”. Finally, I looked for questions using verbs in the commonsense knowledge base VerbPhysics ([Forbes and Choi, 2017](https://arxiv.org/pdf/1706.03799.pdf)). VerbPhysics uses verbs to figure out the relative size, weight, speed, strength, and rigidness of pairs of objects. For example, if x throws y, x is likely larger than y. For all of these categories, I sorted for the questions that included these words and found their accuracies using the pre-trained Google CoLab VQA model. 

We wanted to see if these questions using physical attributes in their reasoning performed worse than the average accuracy. However, I found some challenges in running these tests. First, sorting questions using words related to physical attributes returned many questions that did not use inter-class comparisons, which would have made it more difficult to narrow down which questions were relevant. For example, questions such as “Is this a speed train?” and “what color is the bigger cat?” include the words “speed” and “bigger”, but do not require the inter-class knowledge gained from VerbPhysics to answer. Furthermore, these categories of questions didn’t seem to do better or worse than the average question using the CoLab model. Finally, although our plan had been to integrate physical common sense knowledge into scene graphs, many of the object pairs in scene graphs did not exist in VerbPhysics, so the knowledge integration would have been sparse. Although we found these challenges, it was satisfying to get my hands dirty and actually explore the datasets outside their papers. Given the challenges of the commonsense project, we decided to focus on the compositional video question answering project this summer, which I will describe in more detail next week. 

Reflecting on week 3, I’d like to take a moment to appreciate the HCI and SVL communities at Stanford. I’ve started attending their weekly meetings and presentations, which have been very interesting. This week, we learned about how to keep medical data secure while still being able to use it to train models. I really appreciate how often they all check in on eachother and how much it feels like a community. I also appreciate how open they are to letting undergrads listen and contribute. I also appreciate how vocal people have been about addressing racial inequities within their own department, academia, and the country at large. Computer vision is a field with a lot of ethical implications, so I’m glad it is openly discussed. I do wish I could have been there this summer, but I hope one day I’ll be able to go in person. 

