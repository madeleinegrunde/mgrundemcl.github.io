---
layout: post
title: Week 8
---

My work the past few weeks has been exciting as the template generation pipeline came together and progress was moving quickly. However, as I test these templates I have been increasingly finding exceptions, qualifications, and challenges with the data as it currently stands. Although they are all fixable, it has been important for me to keep track of all the exceptions I find and systematically avoid them. 

One of the major challenges was that the Action Genome dataset had many instances of annotated objects without any relationship annotations. Therefore, when I asked a simple question like “Did they hold the dish?”, the generated answer was correct. However, if I specified what part of the video to look at like “Did they hold the dish after putting down the towel?” The answer would be incorrect because the Action Genome annotations only said they were holding the dish for one frame. I accounted for these errors in two ways. First, I split the Charades annotations into their corresponding verbs and subjects. For each of these splits I added a relationship node to the spatio-temporal scene graph based on the verb and connected it to its subject annotation. Second, I listed what verbs entailed one another. For example, if you are carrying something you are also holding it, and you are also touching it. So for each frame where “carrying” was a relationship node, I also added “holding” and “touching” relationships. Along with these and some other exceptions, I used this week to expand the suite of indirect references and program modules. 

This week’s presentations were also interesting. The HCI lunch presentation discussed a platform called Gradio [1] that helps domain experts interact with AI systems in their domain. For example, with this platform a developer making a system to use in the medical field can upload their ML model to Gradio, then the doctors who are experts in the domain can upload test images and see with what the model does well and with what it struggles. Another presentation I watched was by Eytan Adar [2] on creating AI to facilitate learning. The particular project they discussed was generating causal graphs from science textbooks using highlighted text. The most interesting part for me was how they had to build in some level of difficulty with using the program, because the goal was not creating the causal graphs but instead facilitating learning. The system would be less effective if it automatically generated a causal graph based on the paragraph text, but they purposefully made it depend on highlighting to improve retention. 

Finally, this week I worked on creating my first official slide deck about the project, which I will present at a meeting on Monday with Ranjay’s advisors. 

[1] https://gradio.app/

[2] https://www.si.umich.edu/people/eytan-adar
