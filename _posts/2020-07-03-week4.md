---
layout: post
title: Week 4
---

This week, I finally started focusing on one project and moving full speed ahead! I’ve really enjoyed working on it so far and I have a feeling I will learn a ton. I am a bit concerned about how much I’ll be able to finish within the 10 week DREU program, since it is an ambitious project. However, I am excited to try and get as far as I can.

The goal of my project is to improve question answering about videos. Visual Question Answering is a popular task to judge a model’s higher reasoning ability by asking questions about an image and judging the model’s answer. Models trained on images then transferred to videos perform poorly, so there is a need for large Video QA datasets. Most models and datasets about Video QA either reference very short videos, or use movie or tv scenes and integrate dialogue. The current literature on Video QA is much smaller than that of VQA because videos are much more computationally expensive to understand. Due to annotation costs, Video QA datasets are also not as large as Image QA datasets. Our project combines 3 ideas to address these issues, 1- generating questions from scene graphs 2- Compositionality, and 3- neural module networks and neural state machines.

## 1- Generating questions from scene graphs. 

Scene graphs are compact ways of representing images as just the objects (man, ball, ocean), their attributes (yellow, standing, empty), and the relationships between them (holding, looking at, in front of) ([Krishna et al, 2017](https://visualgenome.org/static/paper/Visual_Genome.pdf)). These scene graphs reduce the image to its most important parts, and have helped with improving many visual reasoning tasks. [Hudson and Manning (2019)](https://arxiv.org/pdf/1902.09506.pdf) made the GQA dataset to use the scene graphs in Visual Genome to automatically generate questions and answers. They created question patterns such as “What color is the <object>?”. They then traversed the scene graphs in Visual Genome to fill in these templates. For example, every object with a color attribute would be filled into the template (e.g. “What color is the apple?”) with the corresponding attribute as the answer (“green”). This approach, unlike manual annotation, allowed the authors to have a stronger sense of the distribution of question types and answers, and to balance the dataset. This balancing helps reduce the statistical biases present in many other VQA datasets, like having the question. For example, people are more likely to ask “Is there a X in the picture?” if X does exist.

Just as scene graphs represent images, spatio-temporal scene graphs (STSGs) represent videos. Like scene graphs, they include objects, attributes, and relationships in select frames throughout the video, however they also connect the same objects and attributes throughout time to represent both continuity and changes. This project will run a similar question-generation pipeline as GQA, but on STSGs instead of image scene graphs. Although we are inspired by the steps in their pipeline, this project will require creating entirely new templates that include the temporal domain, expanding STSGs to a useable structure, building infrastructure to traverse STSGs, and defining and building a more complex system to automatically determine answers by reasoning across time. 

## 2- Compositionality

Some questions about images require multiple steps of reasoning in order to answer. For example, the question “What color is the apple on the blue plate?” is composed of subtasks, specifically locating all plates, finding the blue plate, looking on top of the plate, finding the apple, and then, finally, determining the color of the apple. Questions that are “composed” of such subtasks are called “compositional questions”. Compositional questions are valuable because they require the models to make multiple steps in reasoning. GQA is able to create many compositional questions by using both direct and indirect references to objects. Each question template was also associated with a semantic “program” that used much more constrained language to dissect the subtasks needed to get the answer (in the example above select(plates) → filter(blue) → select(apple) → query(color)). 

We argue that questions about video are often inherently compositional. Many questions need to localize a point in time in reference to some action, then ask a question about that point in time. Even simple questions (e.g.  “Is there a cup in the video?”) require traversing through the entire video while looking for a cup. Therefore, we want to create challenging questions about videos with multiple layers of compositionality and semantic programs to go with each. 

## 3- Neural State Machines

These semantic programs can be represented as “modules” in a neural module network. Each module performs a specific task (like selecting objects only with some attribute), and they can be built in different forms to answer a variety of questions. Neural state machines answer questions using modules. However, instead of having each module shift attention over parts of the images, the modules shift attention over a scene graph representation ([Hudson and Manning, 2019](https://arxiv.org/pdf/1907.03950.pdf)). We would like to train modules to shift attention over STSGs. We hypothesize that answering questions about videos by shifting attention over STSGs will reduce the massive input of a video into just its important parts, and therefore make it easier to train a network. 

I am very excited about this project. It has been interesting to try to break time down into its basic parts, then combine those parts in different ways to represent different higher level concepts.

In other news from this week, we had an especially interesting SVL talk about projects integrating vision systems into various healthcare settings. The two that stood out to me the most used vision to allow for seniors to be more independent and for therapists to better diagnose and care for their patients. I thought it was so cool that they were able to take visual and language tools and find trends that would have been near impossible to see from a purely experimental or observational study. For example, they looked at people’s facial expressions and body movements to diagnose depression, and they found phrases that were more effective for therapists to use than others.

The presenters brought up security risks and were explicit about how they tried to mitigate any privacy concerns. However, I do wonder how people would react to allowing these systems into their life and into the healthcare environments, and what unintended biases they will incorporate. I think there is a lot to be gained from integrating these technologies if they come to be, but these talks have continued to fuel my interest in understanding how the application of these technologies into interactions with professionals and their patients will differ from their applications in the lab. 
