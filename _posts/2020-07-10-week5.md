---
layout: post
title: Week 5
---

This week I focused on building out the dataset creation pipeline and defining programs and question templates. It was a busy, but very satisfying week, seeing the pipeline slowly come together

Last week I had begun creating the graph representation. At first I represented the scene graph as a 3D array of (objects x relationships x frame), where (x, y, z) was 1 if a person was doing relationship y to object x in frame z and 0 otherwise. However, this data structure ended up being difficult to navigate for multi-step questions. It also required a separate array to keep track of the overall actions. Instead, we’ve represented STSGs as dictionaries. The STSGs include vertices for objects, relationships, actions, and frames. Each of these vertices has references to the other ones in the same frame, as well as its next and previous occurrences in other frames of the video. This structure allows the program to hop through the dictionary to easily find the answers to complex questions. 

With this dictionary set up, I began creating a few templates. Although I brainstormed enough templates to make sure that our program-based approach was flexible, I decided to make sure the entire pipeline worked with the data structures I chose before coding a ton of question templates. It was an iterative process as I went through the rest of the pipeline, in which as I realized later on different things I needed, I adjusted both the templates and the graph representation itself. The rest of the pipeline involved traversing through the graph to fill the templates, adjusting questions to have correct grammar, building modules to answer the questions, balancing the distribution of different types of answers, and visualizing the resulting distribution. Although it took a lot of work the past two weeks, at the end of week 5, I have close to the whole pipeline set up. My next steps are to add in indirect references and to start adding more templates. There is a lot to do for my project in the coming weeks, but I do want to make sure in the future that I still take time to explore and read papers not immediately related to my project. 

This week’s zoom meetings with the other SVL and HCI members were especially interesting. Olga Russakovsky spoke about her work on finding, defining, and addressing dataset biases. Some biases she mentioned were more complex to fix than just the percentage of images with a man or with a white person. For example, the types of pictures that include women with flowers are very different from the types of pictures that include men with flowers. This means that just addressing the percentage of photos with a person with flowers that are men and women would not make the “flowers and person” subset of the dataset free of any gender bias. I especially liked her platform for anyone to use where they could input their dataset and find potential biases in it. That’s so cool. I like how her work is both very intensive in how it conceptualizes and processes datasets, yet it is also focused towards using that to make understandable tools to help researchers do better. 

The other interesting meeting I did not get to attend, but I discussed its contents with some coworkers and read the slides. It was by Michael Bernstein about the idea of “velocity” in research. He believes that nearly every research project has something called “the swamp” which bogs you down and hurts the progress of the project. He explained that he measures progress by the amount of targeted actions done in creative ways to try to fix the most insecure part of the project. In other words, he does not judge progress by how many tasks off the overall project were finished, but instead by how many different, creative ways were tried on a targeted part of the problem. Even if they failed. He believes that spending forever formulating the perfect idea before approaching the project is worse than formulating the idea as you move through the project, because the project itself will teach you a lot about its possibilities and downfalls that hypothesizing could not. I do think that I can get caught up in trying to engineer the perfect bit of code to do some task in the most optimal way, so it is a good reminder that there needs to be a balance between making your life easier in the future and focusing on solving the most critical problem at hand rather than spending a lot of time creating the perfect version of something you already know works. 

I’m looking forward to continuing high velocity research next week!
